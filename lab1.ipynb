{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5lzpQrIl2UL"
      },
      "source": [
        "# Lab1 - Scikit-learn\n",
        "Author: *Samuel Sofela*\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "The goal of this lab is to become familiar with the scikit-learn library.\n",
        "\n",
        "You will practice loading example datasets, perform classification and regression with linear scikit-learn models, and investigate the effects of reducing the number of features (columns in X) and the number of samples (rows in X and y).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GyDhYsyXl2UM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXeOQtGZl2UM"
      },
      "source": [
        "## 2. Classification\n",
        "\n",
        "Using yellowbrick spam - classification  \n",
        "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
        "\n",
        "The goal is to investigate `LogisticRegression(max_iter=2000)` and effects of reducing the number of features and number of samples on classification performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Nvc1EBml2UM"
      },
      "source": [
        "### 2.1 Implement convenience function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BaOhivSvl2UM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_classifier_accuracy(model, X, y):\n",
        "    '''Calculate train and validation accuracy of classifier (model)\n",
        "        \n",
        "        Splits feature matrix X and target vector y \n",
        "        with sklearn train_test_split() and random_state=956.\n",
        "        \n",
        "        model (sklearn classifier): Classifier to train and evaluate\n",
        "        X (numpy.array or pandas.DataFrame): Feature matrix\n",
        "        y (numpy.array or pandas.Series): Target vector\n",
        "        \n",
        "        returns: training accuracy, validation accuracy\n",
        "    \n",
        "    '''\n",
        "    #TODO: IMPLEMENT FUNCTION BODY\n",
        "    # split the data into training set and validation set\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=956)\n",
        "    \n",
        "    # fit the model using training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # predict the data on training set\n",
        "    y_predicted_training = model.predict(X_train)\n",
        "\n",
        "    # get the training accuracy\n",
        "    training_accuracy = accuracy_score(y_train, y_predicted_training)\n",
        "\n",
        "    # predict the data on validation set\n",
        "    y_predicted_test = model.predict(X_test)\n",
        "    \n",
        "    # get the validation accuracy\n",
        "    test_accuracy = accuracy_score(y_test, y_predicted_test)\n",
        "\n",
        "    return (training_accuracy, test_accuracy)\n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY1xx51Pl2UN"
      },
      "source": [
        "### 2.2 Load data\n",
        "\n",
        "Use the yellowbrick function `load_spam()`, load the spam data set into feature matrix `X` and target vector `y`.\n",
        "\n",
        "Print size and type of `X` and `y`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaDaq_kkl2UN",
        "outputId": "3bfca4c3-532c-464c-c485-6efbcc36c724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4600, 57)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(4600,)\n",
            "<class 'pandas.core.series.Series'>\n",
            "46\n"
          ]
        }
      ],
      "source": [
        "# TODO: ADD YOUR CODE HERE\n",
        "from yellowbrick.datasets.loaders import load_spam\n",
        "X,y = load_spam()\n",
        "print(X.shape)\n",
        "print(type(X))\n",
        "print(y.shape)\n",
        "print(type(y))\n",
        "print(int(len(X)*0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s_jb91Al2UN"
      },
      "source": [
        "Using the sklearn function `train_test_split()` prepare a feature matrix `X_small` and target vector `y_small` that contain only **1%** of the rows. Use `random_state=174`.\n",
        "\n",
        "Print size and type of `X_small` and `y_small`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUgXNQjIl2UO",
        "outputId": "8e68fd87-c5ff-44d3-b4a9-665f451d47cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The side of x_small is:  (46, 57)  and the type is:  <class 'pandas.core.frame.DataFrame'>\n",
            "The side of x_small is:  (46,)  and the type is:  <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# TODO: ADD YOUR CODE HERE\n",
        "x_small,_, y_small, _ = train_test_split(X, y, train_size=0.01, random_state=174)\n",
        "print(\"The side of x_small is: \", x_small.shape, \" and the type is: \", type(x_small))\n",
        "print(\"The side of x_small is: \", y_small.shape, \" and the type is: \", type(y_small))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck3hhlBZl2UO"
      },
      "source": [
        "### 2.3 Train and evaluate models\n",
        "\n",
        "1. Import `LogisticRegression` from sklearn\n",
        "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
        "3. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
        "4. Call your convenience function `get_classifier_accuracy()` using \n",
        "    - `X` and `y`\n",
        "    - Only first two columns of `X` and `y`\n",
        "    - `X_small` and `y_small`\n",
        "5. Add the data size, training and validation accuracy for each call to the `results` DataFrame\n",
        "6. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUVoZBnHl2UO",
        "outputId": "7c35b0f8-aae0-4b7a-e9f7-46514d5f5214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Data size  Training Accuracy  Validation Accuracy\n",
            "0  (4600, 57)           0.934493             0.918261\n",
            "1   (4600, 2)           0.608986             0.613043\n",
            "2    (46, 57)           0.941176             0.750000\n"
          ]
        }
      ],
      "source": [
        "# TODO: ADD YOUR CODE HERE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(max_iter=2000)\n",
        "\n",
        "results_pd = pd.DataFrame(columns = ['Data size', 'Training Accuracy', 'Validation Accuracy'])\n",
        "\n",
        "train_acc, test_acc = get_classifier_accuracy(logreg, X, y)\n",
        "results_pd.loc[0]=[X.shape, train_acc,test_acc ]\n",
        "\n",
        "x_2col = X.iloc[:,:2]\n",
        "train_acc, test_acc = get_classifier_accuracy(logreg, x_2col, y)\n",
        "results_pd.loc[1]=[x_2col.shape, train_acc,test_acc ]\n",
        "\n",
        "train_acc, test_acc = get_classifier_accuracy(logreg, x_small, y_small)\n",
        "results_pd.loc[2]=[x_small.shape, train_acc,test_acc ]\n",
        "\n",
        "print(results_pd)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluating the difference between the training and validation accuracies for the entire data\n",
        "results_pd[\"Difference in Accuracy\"]= results_pd['Training Accuracy'] - results_pd['Validation Accuracy']\n",
        "print(results_pd)\n"
      ],
      "metadata": {
        "id": "CZU4oChS8Vzd",
        "outputId": "031885d2-503f-4e23-84d1-3257b4c58155",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Data size  Training Accuracy  Validation Accuracy  Difference in Accuracy\n",
            "0  (4600, 57)           0.934493             0.918261                0.016232\n",
            "1   (4600, 2)           0.608986             0.613043               -0.004058\n",
            "2    (46, 57)           0.941176             0.750000                0.191176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeEvyxrQl2UO"
      },
      "source": [
        "### 2.4 Questions\n",
        "1. What is the validation accuracy using all data? What is the difference between training and validation accuracy?\n",
        "1. How does the validation accuracy and difference between training and validation change when only two columns are used? Provide values.\n",
        "1. How does the validation accuracy and difference between training and validation change when only 1% of the rows are used? Provide values.\n",
        "\n",
        "*YOUR ANSWERS HERE*\n",
        "\n",
        "1. Using all the data, Validation accuracy= 0.918261, and the difference between the training data and validation data is: 0.016232\n",
        "2. When only two columns are used, relative to using the entire data,  the validation accuracy reduces from 0.918261 to 0.613043 while the difference between the training and validation accuracies decreases from 0.016232 to -0.004058.\n",
        "3. When only 1% of the rows are used, relative to using the entire data,  the validation accuracy reduces from 0.918261 to 0.750000 while the difference between the training and validation accuracies increases from 0.016232 to 0.191176.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAzUq6NMl2UO"
      },
      "source": [
        "## 3. Regression\n",
        "\n",
        "Using yellowbrick energy - regression  \n",
        "https://www.scikit-yb.org/en/latest/api/datasets/energy.html\n",
        "\n",
        "The goal is to investigate `LinearRegression()` and effects of reducing the number of features and number of samples on regression performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaHmBrSSl2UP"
      },
      "source": [
        "### 3.1 Implement convenience function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "z5N9Hfdll2UP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_regressor_mse(model, X, y):\n",
        "    '''Calculate train and validation mean-squared error (mse) of regressor (model)\n",
        "        \n",
        "        Splits feature matrix X and target vector y \n",
        "        with sklearn train_test_split() and random_state=956.\n",
        "        \n",
        "        model (sklearn regressor): Regressor to train and evaluate\n",
        "        X (numpy.array or pandas.DataFrame): Feature matrix\n",
        "        y (numpy.array or pandas.Series): Target vector\n",
        "        \n",
        "        returns: training mse, validation mse\n",
        "    \n",
        "    '''\n",
        "    #TODO: IMPLEMENT FUNCTION BODY\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=956)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_train_predicted = model.predict(X_train)\n",
        "    y_test_predicted = model.predict(X_test)\n",
        "    mse_training = mean_squared_error(y_train, y_train_predicted)\n",
        "    mse_validation = mean_squared_error(y_test, y_test_predicted)\n",
        "    return (mse_training, mse_validation)\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3uYNi_4l2UP"
      },
      "source": [
        "### 3.2 Load data\n",
        "\n",
        "Use the yellowbrick function `load_energy()` load the energy data set into feature matrix `X` and target vector `y`.\n",
        "\n",
        "Print dimensions and type of `X` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_9dJGhVl2UP",
        "outputId": "979e5598-5f22-48da-8bce-f9e8468d68ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The side of x is:  (768, 8)  and the type is:  <class 'pandas.core.frame.DataFrame'>\n",
            "The side of x is:  (768,)  and the type is:  <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# TODO: ADD YOUR CODE HERE\n",
        "from yellowbrick.datasets.loaders import load_energy\n",
        "x,y = load_energy()\n",
        "print(\"The side of x is: \", x.shape, \" and the type is: \", type(x))\n",
        "print(\"The side of x is: \", y.shape, \" and the type is: \", type(y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgqzap7ql2UP"
      },
      "source": [
        "Using the sklearn function `train_test_split()` prepare a feature matrix `X_small` and target vector `y_small` that contain only **1%** of the rows. Use `random_state=174`.\n",
        "\n",
        "Print size and type of `X_small` and `y_small`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ezYd1VZl2UP",
        "outputId": "bc7b18cd-29ff-4fa4-ada5-16593d34c718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The side of x_small is:  (7, 8)  and the type is:  <class 'pandas.core.frame.DataFrame'>\n",
            "The side of y_small is:  (7,)  and the type is:  <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# TODO: ADD YOUR CODE HERE\n",
        "x_small, _, y_small, _ = train_test_split(x, y, train_size=0.01, random_state=174)\n",
        "print(\"The side of x_small is: \", x_small.shape, \" and the type is: \", type(x_small))\n",
        "print(\"The side of y_small is: \", y_small.shape, \" and the type is: \", type(y_small))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klLF62Nol2UP"
      },
      "source": [
        "### 3.3 Train and evaluate models\n",
        "\n",
        "1. Import `LinearRegression` from sklearn\n",
        "2. Instantiate model `LinearRegression()`.\n",
        "3. Create a pandas DataFrame `results` with columns: Data size, training MSE, validation MSE\n",
        "4. Call your convenience function `get_regressor_mse()` using \n",
        "    - `X` and `y`\n",
        "    - Only first two columns of `X` and `y`\n",
        "    - `X_small` and `y_small`\n",
        "5. Add the data size, training and validation MSE for each call to the `results` DataFrame\n",
        "6. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzlJ_tKUl2UP",
        "outputId": "2fbe35a7-ec88-44a6-e70d-de64cddeda61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Data size  Training MSE  Validation MSE\n",
            "0  (768, 8)  8.012691e+00       10.366349\n",
            "1  (768, 2)  5.360043e+01       46.410426\n",
            "2    (7, 8)  2.145702e-29       69.977449\n"
          ]
        }
      ],
      "source": [
        "# TODO: ADD YOUR CODE HERE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "linreg = LinearRegression()\n",
        "\n",
        "results_pd1 = pd.DataFrame(columns = ['Data size', 'Training MSE', 'Validation MSE'])\n",
        "\n",
        "train_acc1, test_acc1 = get_regressor_mse(linreg, x, y)\n",
        "results_pd1.loc[0]=[x.shape, train_acc1,test_acc1]\n",
        "\n",
        "x_2col = x.iloc[:,:2]\n",
        "train_acc1, test_acc1 = get_regressor_mse(linreg, x_2col, y)\n",
        "results_pd1.loc[1]=[x_2col.shape, train_acc1 ,test_acc1]\n",
        "\n",
        "train_acc1, test_acc1 = get_regressor_mse(linreg, x_small, y_small)\n",
        "results_pd1.loc[2]=[x_small.shape, train_acc1,test_acc1]\n",
        "\n",
        "print(results_pd1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluating the difference between the training and validation accuracies for the entire data\n",
        "results_pd1[\"Difference in MSE\"]= results_pd1['Training MSE'] - results_pd1['Validation MSE']\n",
        "print(results_pd1)"
      ],
      "metadata": {
        "id": "lppunYOoEL_r",
        "outputId": "7ab13cee-f6ce-4124-91af-dcde60ea9be0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Data size  Training MSE  Validation MSE  Difference in MSE\n",
            "0  (768, 8)  8.012691e+00       10.366349          -2.353657\n",
            "1  (768, 2)  5.360043e+01       46.410426           7.190004\n",
            "2    (7, 8)  2.145702e-29       69.977449         -69.977449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFRkx5ifl2UP"
      },
      "source": [
        "### 3.4 Questions\n",
        "1. What is the validation MSE using all data? What is the difference between training and validation MSE?\n",
        "1. How does the validation MSE and difference between training and validation change when only two columns are used? Provide values.\n",
        "1. How does the validation MSE and difference between training and validation change when only 1% of the rows are used? Provide values.\n",
        "\n",
        "*YOUR ANSWERS HERE*\n",
        "\n",
        "1. Using all the data, Validation MSE= 10.366349, and the difference, (training data - validation data) is: -2.353657\n",
        "2. When only two columns are used, relative to using all the data, the validation MSE increases from 10.366349 to 46.410426 while the difference between training MSE and validation MSE increases from -2.353657 to  7.190004.\n",
        "3. When only 1% of rows are used, relative to using the entire data, the validation MSE increases from from 10.366349 to 69.977449 while the difference between training MSE and validation MSE increases from from -2.353657 to -69.977449 (the absolute values are considered)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se5XzVhVl2UQ"
      },
      "source": [
        "## 4. Observations/Interpretation\n",
        "\n",
        "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
        "\n",
        "\n",
        "*ADD YOUR FINDINGS HERE*\n",
        "1. Using 2 columns implies that we are reducing the model complexities. For the logistic regression, the resulted in decrease in the validation accuracy (from 0.918261 to 0.613043) and the difference between the validation and training accuracies reduces (0.016232 to -0.004058). This show that we are in the high bias regime and aligns with the validation curve in class.\n",
        " \n",
        "  In a similar scenario of reducing model complexity using a linear regression model, the MSE increases from 10.366349 to 46.410426 while the difference in the validation and training MSE increases from -2.353657 to 7.190004. This implies that with reduced complexity, the model predicted the data with more error (higher MSE) which means lesser accuracy as expected. This is also reflected in high MSE of the training data 53.60043. \n",
        "\n",
        "  For both models, the accuracy of the training model reduced with reduced model complexity leading to high bias and is underfit.\n",
        "\n",
        "2. Using 1% of the data set reduces the size of the training set. For the logistic regression model, this increased the difference between the training and validation accuracies from 0.016232 to 0.191176. The validation accuracy also reduced from 0.918261 to 0.750000. This implies that model is in the high variance regime and aligns with the learning curve described in class. \n",
        "\n",
        "  Similarly, reduce the size of the data sample in the linear regression model increased the  MSE -2.353657 to -69.977449 and the validation MSE also increased from 10.366349 to 69.977449. This implies the high variance regime and the low MSEs of the training set relative to the validation set shows that the model was overfit.\n",
        "\n",
        "\n",
        "Overall, the following conclusion can be made:\n",
        "1. Original model = good fit\n",
        "2. model using 2 columns = underfit\n",
        "3. model using 1% data size = overfit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOoC15GZl2UQ"
      },
      "source": [
        "## 5. Reflection\n",
        "Include a sentence or two about:\n",
        "- what you liked or disliked,\n",
        "- found interesting, confusing, challangeing, motivating\n",
        "while working on this assignment.\n",
        "\n",
        "\n",
        "*ADD YOUR THOUGHTS HERE*\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}